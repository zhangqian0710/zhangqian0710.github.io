<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[8.个人学习ES之路-从安装入门到项目之实战window环境入门学习]]></title>
    <url>%2F2019%2F08%2F30%2F8.%E4%B8%AA%E4%BA%BA%E5%AD%A6%E4%B9%A0ES%E4%B9%8B%E8%B7%AF-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%83-%E4%B8%8A%2F</url>
    <content type="text"><![CDATA[个人学习ES之路-从安装入门到项目之实战window环境入门学习大体分三个阶段：1.了解阶段 2.安装配置单例及集群阶段 3.实战学习阶段 4.总结阶段 一、了解阶段：听说是目前最牛逼的搜索插件 二、安装阶段：1.下载ES（6.4.2）ES官网：https://www.elastic.co/cn/方便下载：链接：https://pan.baidu.com/s/18GpfnqR77_aTFVkiYnHyMg 提取码：76yt 2.解压缩后进入bin目录，双击elasticsearch.bat看到下入红色框，则是单例启动成功，进入 localhost：9200 可以看到返回的json串 3.插件安装Head安装github上搜索elasticsearch-head 下载，解压缩后，用cmd界面，进入此目录，执行==npm install==（不支持npm请百度一下） 此时在elasticsearch-head下输入==npm run start==， 发现localhost：9100 后台界面，然后关闭命令。（相当于关闭head程序） 4.配置elasticsearch6-4-2/conf/elasticsearch.yml配置文件#master的conf配置http.cors.enabled: truehttp.cors.allow-origin: “*” cluster.name: zqnode.name: masternode.master: true network.host: 127.0.0.1 【到这一步后，再次运行 elasticsearch.bat 程序 和 head目录下 npm run start ，进入9100界面 能看到master已运行】 5.配置集群：新建两个文件夹 slave1和slave2 ，把最早下载的ES压缩包分别解压缩到这两个文件夹，然后分别修改各自的conf文件（如下）， 然后 分别启动各自es程序， 刷新9100界面可以发现 集群创建完成 #slave1的conf配置： cluster.name: namenode.name: slave1 network.host: 127.0.0.1http.port: 8200 discovery.zen.ping.unicast.hosts: [“127.0.0.1”] #slave2的conf配置： cluster.name: namenode.name: slave1 network.host: 127.0.0.1http.port: 8000 discovery.zen.ping.unicast.hosts: [“127.0.0.1”] 6.基本知识：索引、类型、文档id （数据库、table、一行记录） 7.开始简单操作用postman，put方式创建索引时候出现这个错误 &quot;Rejecting mapping update to [people] as the final mapping would have more than 1 type: [woman, man]&quot; 原因，es6.0版本后 不支持一次性插入多个类型 （1）创建索引：可以走head界面手动创建，或者走postman发起put请求创建：地址==127.0.0.1:9200/people==12345678910111213141516171819202122232425&#123; "settings": &#123; "number_of_shards": 3, "number_of_replicas": 1 &#125;, "mappings": &#123; "man": &#123; "properties": &#123; "name": &#123; "type": "text" &#125;, "country": &#123; "type": "keyword" &#125;, "age": &#123; "type": "integer" &#125;, "date": &#123; "type": "date", "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis" &#125; &#125; &#125; &#125;&#125; （2）数据插入指定id插入：put请求、 postman地址:127.0.0.1:9200/people/man/1 (索引/类型/id 必须给id) 12345678&#123; "name": "zq", "country": "China", "age": " 22", "date": "1997-07-10"&#125; 自定id插入：post请求、 postman地址:127.0.0.1:9200/people/man/ (索引/类型/ 可以不指定id) 12345678&#123; "name": "zqnb", "country": "China", "age": "18", "date": "2019-07-10"&#125; 效果图： （3）数据修改post请求 ：postman 请求地址为： 127.0.0.1:9200/people/man/id/ ==_update== 12345&#123; "doc": &#123; "name": "zq666" &#125;&#125; ==脚本修改方式：== 127.0.0.1:9200/people/man/id/ ==_update== 123456789&#123; "script": &#123; "lang": "painless", "inline": "ctx._source.age += params.age", "params": &#123; "age": 100 &#125; &#125;&#125; （4）删除删除数据：delete 请求 127.0.0.1:9200/people/man/1/ 删除索引： head界面直接操作 或者 127.0.0.1:9200/people （5）查询简单查询： get请求 127.0.0.1:9200/people/man/id 条件查询：post请求 127.0.0.1:9200/people/man/id/ _search 【match 匹配】 1234567&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "from": 1, "size": 1&#125; 1234567&#123; "query": &#123; "match": &#123; "title": &#123;XXX&#125; &#125; &#125;&#125; 需要排序的 字段 12345678910&#123; "query": &#123; "match": &#123; "title": &#123;XXX&#125; &#125; &#125;, "sort": &#123; &#123;"publish_date": &#123;"order": "desc"&#125;&#125; &#125;&#125; 聚合查询： 1234567891011121314&#123; "aggs": &#123; "group_by_word_count": &#123; "terms": &#123; "field": "word_count" &#125; &#125;, "group_by_publish_date": &#123; "terms": &#123; "field": "publish_date" &#125; &#125; &#125; &#125; 最大最小等查询： 123456789&#123; "aggs": &#123; "grades_word_count": &#123; "stats": &#123; "field": "word_coun" &#125; &#125; &#125; &#125; 模糊查询：（match 包含一部分就可以 match_phrase包含整个 multi_match 多个条件模糊查询） 1234567&#123; "query": &#123; "关键字": &#123; "author": "XX" &#125; &#125; &#125; 语法查询：包含： “query”: “XX and XX” 包含多个： “query”: “（XX and XX） or XXXX” 包含多个字段： 1234567&#123; "query": &#123; "query_string": &#123; "query": "XX and XX" &#125; &#125; &#125; 包含多个字段： 12345678&#123; "query": &#123; "query_string": &#123; "query": "XX or XX" "fields": ["title, "author"] &#125; &#125; &#125; 具体化查询： 1234567&#123; "query": &#123; "term": &#123; “XX”： “XX” &#125; &#125; &#125; 范围查询： gt lt 大于小于支持 数字 时间 类型、 now关键词代表 今天 12345678910&#123; "query": &#123; "range": &#123; “word_count”： &#123; “gte": 100， "lte": 200 &#125; &#125; &#125; &#125; 子条件查询【过滤查询】：filter context 1234567891011&#123; "query": &#123; "bool": &#123; “filter": &#123; “term": &#123; "word_count": 100 &#125; &#125; &#125; &#125; &#125; 符合条件查询： 固定分数查询： 个人github：https://github.com/zhangqian0710?tab=repositories 个人csdn:https://mp.csdn.net/mdeditor/96853700#]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[7.无聊写写之java小技巧]]></title>
    <url>%2F2019%2F08%2F20%2F7.%E6%97%A0%E8%81%8A%E5%86%99%E5%86%99%E4%B9%8Bjava%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[无聊写写之java小技巧1.通过java反射获得java某个类中的方法名字：1Thread.currentThread().getStackTrace()[1].getMethodName(); 2.比较某一天时间和系统当前日期的大小，比系统日期大返回1，否则返回012345678910public static int timeIsBig(Date time)&#123; DateFormat df = new SimpleDateFormat("yyyy-MM-dd"); String dt1 = df.format(time); String dt2 = df.format(new Date()); if (dt1.compareTo(dt2) &gt;= 0) &#123; return 1; &#125; else &#123; return 0; &#125; &#125; 3.解析IP地址（获取country国家、region地区、province省份、city城市、operator服务商）获得成为一个mapip.db下载：https://github.com/lionsoul2014/ip2region （来源网络） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576 //本地创建ip.db文件，通过FileUtils.copyInputStreamToFile把ip2region.db内容通过流复制 String tmpDir = System.getProperties().getProperty("java.io.tmpdir"); String dbPath = tmpDir + "ip.db"; try &#123; File file = new File(dbPath); FileUtils.copyInputStreamToFile(IPUtil.class.getClassLoader().getResourceAsStream("ip2region.db"), file); //查询算法 int algorithm = DbSearcher.BTREE_ALGORITHM; //B-tree //DbSearcher.BINARY_ALGORITHM //Binary //DbSearcher.MEMORY_ALGORITYM //Memory DbConfig config = new DbConfig(); DbSearcher searcher = new DbSearcher(config, dbPath); //define the method Method method = null; switch (algorithm) &#123; case DbSearcher.BTREE_ALGORITHM: method = searcher.getClass().getMethod("btreeSearch", String.class); break; case DbSearcher.BINARY_ALGORITHM: method = searcher.getClass().getMethod("binarySearch", String.class); break; case DbSearcher.MEMORY_ALGORITYM: method = searcher.getClass().getMethod("memorySearch", String.class); break; &#125; DataBlock dataBlock = null; if (Util.isIpAddress(ip) == false) &#123; System.out.println("Error: Invalid ip address"); &#125; dataBlock = (DataBlock) method.invoke(searcher, ip); String[] strArray = dataBlock.getRegion().split("\\|"); Map ipAddressData = new HashMap(); //国家 if ("0".equals(strArray[0])) &#123; ipAddressData.put("country", null); &#125; else if("澳门".equals(strArray[0]) || "香港".equals(strArray[0]) || "台湾".equals(strArray[0]))&#123; ipAddressData.put("country", "中国"); &#125;else &#123; ipAddressData.put("country", strArray[0]); &#125; //地区 if ("0".equals(strArray[1])) &#123; ipAddressData.put("region", null); &#125;else &#123; ipAddressData.put("region", strArray[1]); &#125; //省份 if ("0".equals(strArray[2])) &#123; ipAddressData.put("province", null); &#125; else &#123; ipAddressData.put("province", strArray[2]); &#125; //地市 if ("0".equals(strArray[3])) &#123; ipAddressData.put("city", null); &#125; else &#123; ipAddressData.put("city", strArray[3]); &#125; //运营方 if ("0".equals(strArray[4])) &#123; ipAddressData.put("operator", null); &#125; else &#123; ipAddressData.put("operator", strArray[4]); &#125; return ipAddressData; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125;&#125; =============================================== 个人github：https://github.com/zhangqian0710?tab=repositories 个人csdn:https://mp.csdn.net/mdeditor/96853700#]]></content>
      <categories>
        <category>知识点总结</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[6.配置Mysql读写分离方法]]></title>
    <url>%2F2019%2F08%2F15%2F6.%E9%85%8D%E7%BD%AEMysql%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[配置Mysql读写分离方法1.在db.properties文件下，配置你的读写库等多个库的配置（包含数据库的url、username、password）12345678910#写库url=jdbc:mysql://xx.xx.xx.xx:3306/（库名）Unicode=true&amp;characterEncoding=utf8&amp;autoReconnect=true&amp;tinyInt1isBit=false&amp;allowMultiQueries=truemysqlUsername=XXXmysqlPassword=XXXX#读库read.url=jdbc:mysql://XX.XX.XX.XX:3306/（库名）Unicode=true&amp;characterEncoding=utf8&amp;allowMultiQueries=true&amp;serverTimezone=Asia/Shanghairead.mysqlUsername=XXXread.mysqlPassword=XXXX 这个文件注意一点，不同库配置的url、username、password 前面的 名字不能相同，以免在spring配置文件中起不到读写分离效果或者冲突。 2.在srping配置文件中，配置数据库事务，和数据源的bean（读写库通用的bean，其中${}参数都是通用）123456789101112131415161718192021222324252627282930313233&lt;!-- 使用annotation定义事务 --&gt; &lt;tx:annotation-driven transaction-manager="transactionManager" proxy-target-class="true"/&gt; &lt;!-- 动态数据源配置文件 --&gt;&lt;context:property-placeholder ignore-unresolvable="true" location="classpath:db.properties"/&gt;&lt;!-- 阿里 druid数据库连接池基础配置 --&gt;&lt;bean id="parentDataSource" abstract="true" class="com.alibaba.druid.pool.DruidDataSource" destroy-method="close"&gt; &lt;!-- 数据库基本信息配置 --&gt; &lt;property name="driverClassName" value="$&#123;driverClassName&#125;" /&gt; &lt;property name="filters" value="$&#123;filters&#125;" /&gt; &lt;!-- 最大并发连接数 --&gt; &lt;property name="maxActive" value="$&#123;maxActive&#125;" /&gt; &lt;!-- 初始化连接数量 --&gt; &lt;property name="initialSize" value="$&#123;initialSize&#125;" /&gt; &lt;!-- 配置获取连接等待超时的时间 --&gt; &lt;property name="maxWait" value="$&#123;maxWait&#125;" /&gt; &lt;!-- 最小空闲连接数 --&gt; &lt;property name="minIdle" value="$&#123;minIdle&#125;" /&gt; &lt;!-- 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒 --&gt; &lt;property name="timeBetweenEvictionRunsMillis" value="$&#123;timeBetweenEvictionRunsMillis&#125;" /&gt; &lt;!-- 配置一个连接在池中最小生存的时间，单位是毫秒 --&gt; &lt;property name="minEvictableIdleTimeMillis" value="$&#123;minEvictableIdleTimeMillis&#125;" /&gt; &lt;property name="validationQuery" value="$&#123;validationQuery&#125;" /&gt; &lt;property name="testWhileIdle" value="$&#123;testWhileIdle&#125;" /&gt; &lt;property name="testOnBorrow" value="$&#123;testOnBorrow&#125;" /&gt; &lt;property name="testOnReturn" value="$&#123;testOnReturn&#125;" /&gt; &lt;property name="maxOpenPreparedStatements" value="$&#123;maxOpenPreparedStatements&#125;" /&gt; &lt;!-- 打开removeAbandoned功能 --&gt; &lt;property name="removeAbandoned" value="$&#123;removeAbandoned&#125;" /&gt; &lt;!-- 1800秒，也就是30分钟 --&gt; &lt;property name="removeAbandonedTimeout" value="$&#123;removeAbandonedTimeout&#125;" /&gt; &lt;!-- 关闭abanded连接时输出错误日志 --&gt; &lt;property name="logAbandoned" value="$&#123;logAbandoned&#125;" /&gt;&lt;/bean&gt; 3.在这个bean下方配置读写库数据源的bean，才用继承上面通用bean的方法，实现整体数据源的配置（${}里面的值是db.properties文件中。）1234567891011121314&lt;!-- 本项目主数据库配置 写库 --&gt; &lt;bean id="writeDataSource" parent="parentDataSource"&gt; &lt;!-- 数据库写库信息配置 --&gt; &lt;property name="url" value="$&#123;url&#125;" /&gt; &lt;property name="username" value="$&#123;mysqlUsername&#125;" /&gt; &lt;property name="password" value="$&#123;mysqlPassword&#125;" /&gt; &lt;/bean&gt; &lt;!-- 本项目主只读库配置 读库 --&gt; &lt;bean id="readDataSource" parent="parentDataSource"&gt; &lt;!-- 数据库只读库信息配置 --&gt; &lt;property name="url" value="$&#123;read.url&#125;" /&gt; &lt;property name="username" value="$&#123;read.mysqlUsername&#125;" /&gt; &lt;property name="password" value="$&#123;read.mysqlPassword&#125;" /&gt; &lt;/bean&gt; 4.配置动态数据源bean，根据service接口上的注解取决于用那个库（1）在spring配置文件中创建bean123456789101112&lt;!-- 动态数据源，根据service接口上的注解来决定取哪个数据源 --&gt; &lt;bean id="dataSource" class="com.XXXXXXX.common.spring.DynamicDataSource"&gt; &lt;property name="defaultTargetDataSource" ref="writeDataSource"/&gt; &lt;property name="targetDataSources"&gt; &lt;map key-type="java.lang.String"&gt; &lt;!-- write or slave --&gt; &lt;entry key="write" value-ref="writeDataSource"/&gt; &lt;!-- read or master --&gt; &lt;entry key="read" value-ref="readDataSource"/&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt; （2）编写几个java文件完成注解的操作（原理上采用每次调用注解，每次获取注解对应的数据源，然后用完后会清楚数据源）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263//获取@DataSource注解@Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD)public @interface DataSource &#123; String value();&#125;//public class DynamicDataSource extends AbstractRoutingDataSource&#123; private Logger logger = LoggerFactory.getLogger(DynamicDataSource.class); @Override protected Object determineCurrentLookupKey() &#123; String dataSouce = DynamicDataSourceHolder.getDataSouce(); //logger.info("dataSource : " + dataSouce); return dataSouce; &#125; //------------------ public class DynamicDataSourceHolder &#123; private static Logger logger = LoggerFactory.getLogger(DynamicDataSourceHolder.class); //写库对应的数据源key private static final String WRITE = "write"; //读库对应的数据源key private static final String READ = "read"; public static final ThreadLocal&lt;String&gt; holder = new ThreadLocal&lt;String&gt;(); public static void putDataSource(String name) &#123; //logger.info("DynamicDataSourceHolder set database: " + name); holder.set(name); &#125; public static String getDataSouce() &#123; //logger.info("DynamicDataSourceHolder get database: " + holder.get()); return holder.get(); &#125; /** * 清除数据源 */ public static void clearDataSource()&#123; holder.remove(); &#125; /** * 标记写库 */ public static void markWrite()&#123; putDataSource(WRITE); &#125; /** * 标记读库 */ public static void markRead()&#123; putDataSource(READ); &#125; &#125; 6.在service层可以采用注解的方式：@DataSource(“read”) 或者@DataSource(“write”)例如: 123456789@DataSource("write") public DayRead getBookDayRead(Map&lt;String, Object&gt; parameter)&#123; return this.dayReadDao.getBookDayRead(parameter); &#125; @DataSource("read") public DayExercise getExerciseDayRead(Map&lt;String, Object&gt; parameter)&#123; return this.dayReadDao.getExerciseDayRead(parameter); &#125; =============================================== 个人github：https://github.com/zhangqian0710?tab=repositories 个人csdn:https://mp.csdn.net/mdeditor/96853700#]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[5.日常无聊总结002]]></title>
    <url>%2F2019%2F08%2F02%2F5.%E6%97%A5%E5%B8%B8%E6%97%A0%E8%81%8A%E6%80%BB%E7%BB%93002%2F</url>
    <content type="text"><![CDATA[日常无聊总结002-Shiro框架学习1.基础学习三个核心组件：Subject, SecurityManager 和 Realms. Subject：即“当前操作用户”。但是，在Shiro中，Subject这一概念并不仅仅指人，也可以是第三方进程、后台帐户（Daemon Account）或其他类似事物。它仅仅意味着“当前跟软件交互的东西”。但考虑到大多数目的和用途，你可以把它认为是Shiro的“用户”概念。 Subject代表了当前用户的安全操作，SecurityManager则管理所有用户的安全操作。 SecurityManager：它是Shiro框架的核心，典型的Facade模式，Shiro通过SecurityManager来管理内部组件实例，并通过它来提供安全管理的各种服务。 Realm： Realm充当了Shiro与应用安全数据间的“桥梁”或者“连接器”。也就是说，当对用户执行认证（登录）和授权（访问控制）验证时，Shiro会从应用配置的Realm中查找用户及其权限信息。 2.shiro框架运行原理图（来自网络） 3.shiro框架和sping security框架的比较： （1）shiro框架比sc框架用起来更简单灵活，不用依赖于其他框架（例如spring）就可以独立运行。 （2）srping security 除了不能脱离Spring，shiro的功能它都有。而且Spring Security对Oauth、OpenID也有支持,Shiro则需要自己手动实现。（可以带上srping 看着就很牛X的一个框架） （3）shiro可以把权限细化到按钮层面（在数据库存储一个用户的所有权限，包括登录权限、菜单展示权限以及细化到每个按钮的权限） 【详细使用步骤见大佬CSDN：https://www.cnblogs.com/jpfss/p/8352031.html 】 =============================================== 4.尽量少用Arrays.asList(“x”,”xx”)方法 123List&lt;String&gt; list = Arrays.asList("a", "b", "c");list.add("c");System.out.println(list.toString()); add，clear，remove的时候会抛出java.lang.UnsupportedOperationException异常，原因如下： 调用asList方法，返回一个ArrayList，但是这个ArrayList不是真的ArrayList，而是Arrays类中的，它自己没有实现add（）、remove（）等方法，继承父类AbstactList中实现的add方法直接抛出了异常，如下图： 所以综上所述，尽量不要使用Arrays.asList()方法将数组转化成字符串。 5.转化成字符串的方法为： （1）非要使用Arrays.asList（）的方法，这样可以变成真正的ArrayList （2）正常循环方法 12345678910//需要转换的数组 String[] arrays = new String[]&#123;"aa","bb","cc"&#125;; //初始化list List&lt;String&gt; list = new ArrayList&lt;String&gt;(); //使用for循环转换为list for(String str : arrays)&#123; list.add(str); &#125; //打印得到的list System.out.println(list); =============================================== 个人github：https://github.com/zhangqian0710?tab=repositories 个人csdn:https://mp.csdn.net/mdeditor/96853700#]]></content>
      <categories>
        <category>知识点总结</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[4.企业文化心得体会001]]></title>
    <url>%2F2019%2F07%2F26%2F4.%E4%BC%81%E4%B8%9A%E6%96%87%E5%8C%96%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A001%2F</url>
    <content type="text"><![CDATA[企业文化之十二条令：（内容来自公司企业文化书籍~）（1）指令：确认指令、及时报告、亲撰周报 （2）行动：说到做到、保持准时、解说问题 （3）汇报：三条总结、一页报告、统计分析 （4）沟通：日清邮件、会议记录、写备忘录 个人心得：一个公司就像是一支军队，类似于现在公司，有指挥人员（管理岗），冲锋手（销售大佬），阵地机枪手（程序猿开发工程师）等。 1.确认指令：（1）要及时回复消息，那怕是回复个好的or收到~ （2）对于复杂命令，或者是功能模块的完成给一个心里计划完成的期限。 （未完待续。。。）]]></content>
      <categories>
        <category>工作感悟</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[3.日常无聊总结001]]></title>
    <url>%2F2019%2F07%2F22%2F3.%E6%97%A5%E5%B8%B8%E6%97%A0%E8%81%8A%E6%80%BB%E7%BB%93001%2F</url>
    <content type="text"><![CDATA[Object类的常见方法：1.hashCode（）； 2.equals（）； 1234567Java中equals和==的区别java中的数据类型，可分为两类： （1）基本数据类型，也称原始数据类型。byte,short,char,int,long,float,double,boolean 他们之间的比较，应用双等号（==）,比较的是他们的值。 （2）复合数据类型(类) 当他们用（==）进行比较的时候，比较的是他们在内存中的存放地址，所以，除非是同一个new出来的对象，他们的比较后的结果为true，否则比较后结果为false。 JAVA当中所有的类都是继承于Object这个基类的，在Object中的基类中定义了一个equals的方法，这个方法的初始行为是比较对象的内存地 址，但在一些类库当中这个方法被覆盖掉了，如String,Integer,Date在这些类当中equals有其自身的实现，而不再是比较类在堆内存中的存放地址了。 对于复合数据类型之间进行equals比较，在没有覆写equals方法的情况下，他们之间的比较还是基于他们在内存中的存放位置的地址值的，因为Object的equals方法也是用双等号（==）进行比较的，所以比较后的结果跟双等号（==）的结果相同。 3.clone（）； //注意事项：此方法可以实现对一个类的克隆操作，但是clone（）方法会抛出CloneNotSupportedException。要调用clone()方法，我们需要将调用放在try-catch块中，或者重新抛出异常。 4.toString（）； //线程大家族方法，通常在多线程方法中使用。 5.wait（）；//通常传入一个等待时间的参数，如果需要提前解束，可以调用notify（）方法。 6.notify（）、notifyall（）等 7.finalize（）//通常在JVM的GC机制（垃圾回收）中使用此方法。 ========================================== Mysql常见小问题：count（*）和count（1）的区别，更推荐用count（column列名）： 1.count（）为聚合函数，是对select的结果集进行技术，但是需要参数不为NULL。 但是count（ * ）不关心返回值是否为null都会计算他的count，然而count（1）中的1为恒真表达式，所以count（ * ）和count（1）本质上没有区别。 2.count（column列名）则会判断结果集每一条数据的column列名是否为null再进行count。所以性能远远低于前两者。 3.效率比较： count(*)=count(1)&gt;count(primary key)&gt;count(column列名) 大于小于和like等常见用法： 1.大于小于会导致索引失效，例如：explain select * from tableA where age &gt;= 15 and age &lt;= 18 的type就是ALL。 2.like查询是以%开头，索引失效；以%结尾，索引有效 （理论上说：统计阅读月份天表的时候数据量小：用 “readDay” like “201801%” 效率高于 “readDay” &gt;= “20180101” and “readDay” &lt;= “20180131 ） =========================================== 语法糖：语法糖（Syntactic Sugar），也称糖衣语法，是由英国计算机学家 Peter.J.Landin 发明的一个术语，指在计算机语言中添加的某种语法，这种语法对语言的功能并没有影响，但是更方便程序员使用。简而言之，语法糖让程序更加简洁，有更高的可读性。 我们所熟知的编程语言中几乎都有语法糖。作者认为，语法糖的多少是评判一个语言够不够牛逼的标准之一。 解语法糖：前面提到过，语法糖的存在主要是方便开发人员使用。但其实，Java虚拟机并不支持这些语法糖。这些语法糖在编译阶段就会被还原成简单的基础语法结构，这个过程就是解语法糖 如果你去看com.sun.tools.javac.main.JavaCompiler的源码，你会发现在compile()中有一个步骤就是调用desugar()，这个方法就是负责解语法糖的实现的。]]></content>
      <categories>
        <category>知识点总结</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2.Hadoop安装]]></title>
    <url>%2F2019%2F07%2F18%2F2.Hadoop%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[准备： 采用NAT方式联网 网关地址：192.168.33.1 3个服务器节点IP地址：192.168.33.101、192.168.33.102、192.168.33.103 子网掩码：255.255.255.0 ======================= 添加HADOOP用户 为HADOOP用户分配sudoer权限 同步时间 设置主机名 hdp-node-01 hdp-node-02 hdp-node-03 配置内网域名映射： 192.168.33.101 hdp-node-01 192.168.33.102 hdp-node-02 192.168.33.103 hdp-node-03 配置ssh免密登陆 配置防火墙 （前提在虚拟机中安装好JDK，hadoop依赖于java开发）1上传HADOOP安装包2规划安装目录 /home/hadoop/apps/hadoop-2.6.13解压安装包4修改配置文件 $HADOOP_HOME/etc/hadoop/5最简化配置如下：vi hadoop-env.sh123# The java implementation to use.export JAVA_HOME=/home/hadoop/apps/jdk1.7.0_51 vi core-site.xml12345678910&lt;configuration&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://hdp-node-01:9000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/home/HADOOP/apps/hadoop-2.6.1/tmp&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; vi hdfs-site.xml1234567891011121314151617181920&lt;configuration&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;&lt;value&gt;/home/hadoop/data/name&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;&lt;value&gt;/home/hadoop/data/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.secondary.http.address&lt;/name&gt;&lt;value&gt;hdp-node-01:50090&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; vi mapred-site.xml123456&lt;configuration&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; vi yarn-site.xml1234567891011&lt;configuration&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;hadoop01&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; vi salves123hdp-node-01hdp-node-02hdp-node-03]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[1.Hello World]]></title>
    <url>%2F2019%2F07%2F18%2F1.HolleWorld%2F</url>
    <content type="text"><![CDATA[HelloWorld！]]></content>
  </entry>
</search>
